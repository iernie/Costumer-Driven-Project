% Generated by GrindEQ Word-to-LaTeX 2010 
% LaTeX/AMS-LaTeX

\documentclass{article}

%%% remove comment delimiter ('%') and specify encoding parameter if required,
%%% see TeX documentation for additional info (cp1252-Western,cp1251-Cyrillic)
%\usepackage[cp1252]{inputenc}

%%% remove comment delimiter ('%') and select language if required
%\usepackage[english,spanish]{babel}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[dvips]{graphicx}
%%% remove comment delimiter ('%') and specify parameters if required
%\usepackage[dvips]{graphics}

\begin{document}

%%% remove comment delimiter ('%') and select language if required
%\selectlanguage{spanish} 

\noindent Distance metrics.

\noindent Bit-map is a way of representing a set of objects. It simply translates values from a set into a vector of fixed length, where each value has a specific place. For example over language L=\{a,b,c,d\} for a sets \{a,b,c\} a bit-map will look like [1 1 1 0] where 1${}^{st}$ integer represents a and last integer represents value d. This way it doesn't matter what order the values are arranged and how many values are so set \{d,b\} over same langue L will be [0 1 0 1] where 1${}^{st}$ value still representing value a, or more correctly said, absence of value a.

\noindent Calculating intersection or union over bit-map A and B is as simple as calculating Boolean expressions. Where union can be easily written as (Ai OR Bi) where i is the position in the vector, and the intersection (Ai AND Bi)

\noindent Weighed bit-map is when each of the values is multiplied by its corresponding weight. Let us say on the language L the weights are a=1 b=2 c=4 d=3, since bit-map over set \{a,b,c\} is [1 1 1 0] the weighed bit-map will be [1*weight-a 1*weight-b 1*weight-c 0*weight-d]=[1 2 4 0]. And for set \{d,b\} it will be[0 2 0 3].

\noindent The Manhattan distance function computes the distance that would be traveled to get from one data point to the other if a grid-like path is followed. It can be written as $\sum^n_{d=1}{|x_d-y_d|}$ where d stands for dimension in space N.

\noindent Hamming distance is defined as number of positions in which a source and target vector disagrees. If vectors represented binary, for example as bit-maps then Hamming distance can be calculated as $d_H\left(s,t\right)=\sum_i{s_i\bigoplus t_i}$.

\noindent Levenshtein distance is based on Hamming distance but adds also operands as insertion, deletion and substitution. And can be written as $d_L\left(s,\ t\right)=number\ operands\ used\ to\ make\ s=t$. If s and t are same length $d_L=\ d_H$

\noindent Ontology distances are more based on compute semantic similarity of objects rather than their textual representation. For example distance between \textit{apple} and \textit{orange} is less then between \textit{apple} and \textit{house}. To calculate this distance you need some sort of logical tool like ontological tree. Where every leaf has a logical ancestor for example ancestor for \textit{apple }will be \textit{fruit}.

\noindent 

\noindent Costumer advice

\noindent In the paper "Towards a Similarity Metric for Comparing Machine Readable Privacy Policies", the some of the problems of defining a similarity metric for privacy policies is discussed. One of the topics was how we can divide calculation of similarity between online 3P3 policies in two, local similarity and global similarity. This way we can use simple mathematical formulas in local similarities like Levenshtein distance or ontology distance. And for global similarity we can calculate simple average sum of distances, or weighed average sum of distances, with the second one we can amplify the importance of some attributes.

\noindent 

\noindent Other important topic was how we should use our knowledge to improve distance calculation, for example for recipient category you have values that are better that other, user privacy wise. Recipient like \textit{ours} is much better then \textit{unrelated} or \textit{public}. So distance between \textit{unrelated} or \textit{public} is less then between \textit{ours} and \textit{unrelated}.

\noindent 

\noindent Implementation

\noindent 

\noindent For the starters let's explain components we use when we compare policies to each other. The main object we have is called \textit{policyObject} it contains P3P policy and relative data we find useful.

\noindent 

\noindent Every 3P3 policy is structured by a given standard. The information that is useful for this chapter is statements a P3P policy has. A policy can have many statements, some of them greatly differ from each other, and others are very similar. 

\noindent 

\noindent Each statement is build of four fields: data-collected, purpose, recipient and retention. Purpose recipient and retention can contain one or many given values, the combination of witch describe how the given data-collected maybe used in the future. Data-collected is divided in four major fields: dynamic, user, thirdparty and business. Many different data-types can be collected in one statement.

\noindent 

\noindent Now we can look how we decided to organize this structure. 

\noindent 

\noindent For every data that being described in a statement, we create a \textit{Case}. A \textit{Case} in a\textit{ policyObject} is like a statement in P3P policy. It also contains the same purpose, recipient and retention, but a \textit{Case }can only have one unique data type; this results that one statement can be translated into many cases.

\noindent 

\noindent \textit{PolicyObject }have a list of all its \textit{Cases} and additional information that we find useful like time of the visit, location of the domain, and action decided by Learning algorithm or user.

\noindent 

\noindent Using the advice from our costumer we implemented to levels of similarity local and global. Though for easy ``switching'' between algorithms a distance Metric class have to contain bought.

\noindent 

\noindent We made it easy to change algorithms for distance calculation. The one we created is called bitmapDistance. 

\noindent 

\noindent For the local similarity BitmapDistance generates bitmaps for fields, retention, purpose, and recipient. This eliminate problem that these fields can have different number of attributes.  Before we use Hamming distance we transform bit-map into weighed bit-map by multiplying every attribute by its weight. We decided that bitmap representation would be bad one for data-types. We implemented a sort of string-comparison/ontological-distance that will be explained later. 

\noindent The Hamming distance for weighed bitmap is easy to implement like we said before it can be represented by Boolean expressions. For practical reasons we 1${}^{st}$ apply weights to bitmap and then calculating distance, this way you can say we take difference between cases because for $d_{h_i}\left(s,t\right)$ to be equals 1 in $d_H\left(s,t\right)=\sum_i{s_i\bigoplus t_i}$. One of $s_i\ or\ t_i$ must be 0 and other one 1. If we apply weights to $d_{h_i}\left(s,t\right)$ it will be 1*weight. So if $d_{h_i}\left(s,t\right)$=1*weight that absolute value of difference between $s_i\ or\ t_i$ will be 1*weight as well. Now we just showed that Hamming distance on binary vectors (also called bitmap) equals Manhattan distance on same sets.

\noindent 

\noindent Hamming distance with creation of bitmaps is a fast algorithm with a linear run-time. The easy ways predicting the result and fast run-time is the reasons we choose this for implementation.

\noindent 

\noindent With the help of the way data type is structured in P3P policies we can calculate a data-type distance by just parsing data-type-string. Every data-type string has to start with one of the four previously mentioned fields, then after a dot a sub-field is followed after other dot a sub-sub-field. Let us say we have your simplified ontological tree just within the syntax of data-type-string with an ``invisible'' root node over those four fields. This way when we have to data-type-strings for comparison we can easily count number of ancestors from string A to string B. For instance String a is ''user.home-info.postal'' and B ``user.bdate''. Number of nodes we need to take from stringA to strinB is 3. 

\noindent 

\noindent The weakness is that without weights it's more string comparison then ontological tree.

\noindent 

\noindent We decided that weighting data-types values was outside of scope of this project. We simply gave static values to 1${}^{st}$ nodes from the root(the four main fields) based on our knowledge and interpretation of private policy.

\noindent 

\noindent It is possible to create a weight system so this algorithm will work like a full ontological tree. For example if we add a value for each possible node value/weight so we can simply take difference between StringA's 1${}^{st}$ 2${}^{nd}$ and 3${}^{rd}$ field and respectfully StringB's fields. If the field 1 in A same as B's then distance is of course 0. But this way we can say that some of the fields or sub-fields based on the same depth in your tree can have same values and distance between them will be 0. For example if we want to say that \textit{bdate} and \textit{name }are equally important for user we can give then same value making difference between them 0, but if we want to increase distance between name and home-info we have give them values that have a greater difference like 1 for name and 10 for home-info, creating distance 9 between them.

\noindent 

\noindent Weights are the key to learning and adjusting this algorithm. They are greatly used in our implementation of Hamming distance and can give great results in data-type analysis.

\noindent 

\noindent The global similarity part was not as easy as creating a bitmap. The number of cases a policy can is undefined, and the similarity of those cases can differ a great deal. Your solution was sum of minimal distance of each Case to a case in the other policy. For instance a caseA from policyA have distance 2, 3, 1 to cases in policyB that mean the distance caseA will have distance 1 to policy. 

\noindent 

\noindent 

\noindent Pseudo code will look like:

\noindent Sum=0

\noindent For caseA in PolicyA.Cases

 Min=infinity

 For caseB in PolicyB.Cases

  Dist=Compare caseA to caseB

  If dist$<$min then

   Min=dist

  End

 End

 Sum=sum+min

\noindent End

\noindent Return sum

\noindent 

\noindent By choosing minimal distance between cases we guaranty that if two cases are identical the distance between them will be 0. But it also creates a problem, consider to policies policyA with \{caseA1 caseA2 caseA3\} and policyB with \{caseB1 caseB2\} where every case in A has minimal distance with caseB1. This way the properties of caseB2 will go ``unnoticed`` in the sum of distances between cases. We solved this problem by running algorithm twice but changing places of policy A and B the 2${}^{nd}$ time and simply summing results.

\noindent 

\noindent The weakness of computing this way is that some of the distances will be counted twice, but user-privacy-safety wise is better to count some cases of minimal distance twice then leave a ``dangerous'' or most distant case out.

\noindent 

\noindent There is some variations in this method. You can use maximum/minimum distance between cases or average of the sum. We choose minimum because this way we always try to find a best match between cases and policies. With an algorithm that will minimize error from computing twice, from a to b and b to a, minimum distance with give the best results. But in total picture if you use same algorithm that considers every case for every policy in your database the results will be proportional.


\end{document}

