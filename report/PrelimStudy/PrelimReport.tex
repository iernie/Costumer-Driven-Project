% Activate the following line by filling in the right side. If for example the name of the root file is Main.tex, write
% "...root = Main.tex" if the chapter file is in the same directory, and "...root = ../Main.tex" if the chapter is in a subdirectory.
 
%!TEX root =  

\chapter{Preliminary Study\label{prelim}}


\minitoc

This section describes the preliminary study phase of the project. It focuses on gaining insight into the problem the software system is to solve. For this project, the problem is very well defined from the customer's perspective as such, little time is spent looking into similar products and various possible solutions to the overarching problem of computer aided privacy advice.

A large portion of this time was spent on investigating the proposed solution and the technologies involved, that is Case Based Reasoning (CBR) and the technologies used by it. As the customer, SINTEF ICT, is a research institution, and the project can somewhat simply be viewed as the "implementation" part of a larger research project, the customer could from the start provide relatively clear specification of the system that is envisioned\footnote{The system is outlined in broad terms in the article Inger Anne T{\o}ndel, {\AA}smund Ahlmann Nyre, Karin Bernsmed. Learning Privacy Preferences. 2011 Sixth International Conference on Availability, Reliability and Security, Vienna, 22-26 Aug. 2011. (s. 621-626). Los Alamitos: IEEE Computer Society.}

Another critical work laid down in this phase were choices with respect to project scope and development model. As will become apparent, these choices are strongly related to the nature of the problem statement. The final piece of work in the preliminary study was to settle on the development and communication tools to be used for the project.

\section{Internet Privacy Technology}

This section briefly describes the situation today with respect to the state of Internet privacy, current Internet privacy tools and which needs the project seeks to address.

\subsection{What is Internet Privacy?}\index{Internet Privacy}
Privacy is the means or ability of an individual to seclude himself or information about himself from third parties and selectively control what personal information\footnote{The term \emph{personal information} will here refer to information such as name postal and e-mail address, financial information, social security number and so forth.} about him is to be available. When talking about \emph{Internet privacy}, it is referred to the control over the type and amount of private information that is revealed under a transmission of data over the Internet, and hereunder, who has access to the information. The legal framework setting the boundaries of the service providers' priveliges with private information is very marginal, and typically, the best information available for users is through the provider's \emph{privacy policy}.

The article "User Agents for Matching Privacy Policies with User Preferences"\footnote{Karin Bernsmed, {\AA}smund Ahlmann Nyre and Martin Gilje Jaatun: "User Agents for Matching Privacy Policies with User Preferences", SINTEF ICT.} describes the rationale for a system aiding users in making Internet privacy decisions in terms of a \emph{privacy paradox} - the fact that while most users claim to put great emphasis on privacy, this is not mirrored in their actions.

Social networks such as Facebook provide a good illustration of this paradox and why it is important. Such services have grown to major prominence as a means of communication over the last half decade, and a common trait of these networks is the sharing of more private information than what was common on the "traditional'' Internet. This is all well and fine, as long as the information is shared in the appropriate sphere, but there are obviously issues once your private images or derogatory comments appear in the public sphere, something that has been related through several news stories. 


\subsection{Current Privacy Technology}\label{privTech}\index{Tor Project}
The disproportion between the average Internet user's concern for privacy and the actual control he has over private information referred to as the privacy paradox above, as well as the very fact that privacy is a \emph{fundamental human right} is obviously something that calls for action. While most websites provide a privacy policy, these tend to very long documents written in a obscure "legalesque" language, intended  for protecting the websites' interests than those of users\footnote{Users are in reality faced with a two cost-utility decisions: the first is wether or not to read the policy, which in most cases is clearly answered with a no. The second is given the lack of knowledge about the policy, wether or not to use the service.}. To aid users in understanding the implications of such policies and hence in making informed privacy related decisions, there is a need for tools providing users with information about policies in a format that is accessible to the user. As pointed out in the abovementioned paper, current technologies are often hard to configure and requires domain knowledge from the user or may be more concerned with hiding information\footnote{Ie. \emph{anonymity}. While this certainly aids in protecting privacy, it also renders certain services such as social networks useless, since a sharing a certain level of private information is required for the service to have any meaning. As an example of this approach, see instance the Tor Project: https://www.torproject.org/.}, rather than the consequences of sharing information which is a common activity in online shopping and social networks.

\subsubsection{The Platform for Privacy Preferences Project (P3P}\index{P3P}\index{The Platform for Privacy Preferences Project}
As an aid in this problem, machine readable standards such as P3P have been introduced. P3P seeks to compress the information contained in the privacy policy in an XML document that can be parsed and summarized by computer programs. 

\subsubsection{AT\&T Privacy Bird}\index{AT\&T Privacy Bird}
The AT\&T \emph{Privacy Bird} is mentioned in T{\o}ndel et al. (2011) as an example of current P3P based Internet privacy software. Privacy Bird can parse P3P documents and match a site's privacy policy with the user's preferences. The problem with this however, is that the user has to explicitly state his preferences, which, first of all, is a rather time-consuming endeavor. Secondly, while a user may have clear notions about which information he would share in a \emph{particular} situation, such preferences may be very hard to generalize. It may be hard to give a general statement on privacy preferences in a way that is both simple enough for the user to understand and rich enough to actually describe the problem. Furthermore, preferences may be inconsistent, for instance, while in the general case, the user would not accept privacy terms similar to those of for instance Facebook; however in Facebook's particular case he will accept them nevertheless.

\subsubsection{Other Privacy Agents}
Bernsmed et al. mentions a few other examples of available and suggested approaches to building privacy systems:

\begin{itemize}
\item \textbf{PIPWatch Toolbar} is a browser plugin that is based on users contributing privacy information about pages through the interface.
\item \textbf{Privacy and Identity Mangament for Europe (PRIME)} is an EU project proposing that service providers add an extra layer for privacy sensitive transactions.
\item \textbf{Collaborative Privacy Management}, in contrast to PRIME, proposes a user-centric arhcitecture that employs collaborative filetering.
\end{itemize}

\subsection{How Machine Learning Can Improve Privacy Advice}\index{Machine Learning}

As mentioned above, there are two key issues with current privacy enhancement technology; firstly, most programs require a substantial amount of user knowledge about internet privacy issues as well as their preferences. While users may have a clear idea about what they would accept in a given situation, these rules may be hard to generalize. This means that the user has to put a large amount of effort into configuring the program. Furthermore, many of these programs may be to vocal in the sense that they interrupt the user in his Internet activities. The aim of this project is to move towards a privacy enhancement system that is easy to configure and intervenes as little as possible. There are several reasons to believe that the introduction of machine learning techniques may be helpful in acheiving this goal.

A successful machine learning system will be able to predict user behavior in a given scenario based on knowledge of previous behavior. The system can learn both \emph{supervised} for instance by being fed a set of cases that are not accepted, and by \emph{reinforcement learning}, that is when the user corrects its behavior. The first case may be applicable to initial configuration and for adding "black lists" of sites that are clearly harmful. The second is necessary for the system to adapt to the complex preferences of the user to minimize the misclassifications.

In a research article, SINTEF indicates the appropriateness of the CBR approach for this problem. A particular advantage of the CBR approach is the feedback loop where the system can actually \emph{explain} its choice in terms of similar cases which sets CBR apart from alternative reasoning models such as artificial neural networks. It also allows for better tuning to user response in those cases that the user disagrees with the recommendation.

\section{Case Based Reasoning}\index{Case Based Reasoning}\index{CBR}
Being, the core part of the system to be developed, some time needed to be spent on looking into CBR. In vague terms, CBR is problem solving based on past solutions to previous problems. This approach is similar in many ways to the way humans solve problems, both as domain experts, and in their daily life. For instance, a software engineer, faced with a particular problem, may identify similarities to a problem he has previously solved, using for instance a factory design pattern, so he adopts this solution to the new problem with some modifications. Similarly, a NTNU student, hungry for a late night snack, recalling past experience with favorable opening hours and culinary excellency, heads off to Sesam.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width = .6\textwidth]{PrelimStudy/cbrCycle}
\caption{A Simplified CBR Cycle.}
\label{cbrCycle}
\end{center}
\end{figure}


For computer implementation, CBR is usually formalized in terms of a four step process:

\begin{itemize}
\item Retrieve: 
Given a new site, the agent will retrieve from its knowledge base, the set of cases deemed the most similar to the one at hand. This means, that if presented with the site Facebook, for instance, the agent finds Twitter, Google and LinkedIn to be the sites that have the most similar policy to Facebook.
\item Reuse: 
Look at the decisions made about the cases that were found and adapt this decision to the problem at hand. In this case, the agent needs to see if there are strong enough indications toward a particular behavior with respect to the type of site that is at hand. If for instance, the user has accepted the policies of all the similar sites found, he is also likely to accept that of Facebook.
\item Revise: 
Once a conclusion is reached, it is presented to the user (along with the background for why it is reached). The user may then choose to accept the conclusion, or to overrule it, providing the system with directions as to why it was wrong. This may in turn cause the agent to update its parameters accordingly. 
\item Retain: Finally, the new case is stored to the database along with the user�s decision as a reference for the next time the same site is opened, and as a case to employ when evaluating new sites.
\end{itemize}

As can be seen from this specification, CBR is a very generic approach, and several notions need to be defined before it can be implemented. For instance, we need to settle on a knowledge representation. For most CBR purposes, a frame based approach is taken. To store this knowledge representation, an appropriate database structure must be selected, and a routine for keeping this up to date must be established. Given a representation, one needs to define a notion of \emph{similarity} between cases, and an algorithm to do the actual retrieval. Usually, one would also like this algorithm to provide some measure of the certainty of the results as well.

While SINTEF has proposed some initial ideas for how these details can be implemented, they are very much open problems, and subject to empirical study to find an 'optimal' approach for the actual problem at hand.

\section{Project Scope}
The research nature of this project makes it a very open-ended one. The clearly most important part of the system envisioned by the customer is the CBR engine, that classifies websites based on the user's previous decisions. Implementing this is clearly necessary, regardless of which direction is followed and what limitations are made.

However, once the local CBR engine and auxiliary modules such as parsing P3P documents and so forth are in place, there are several directions that the project can take, and pursuing all of them is an unlikely scenario given the resource limitations. To some extent, the direction of the project also depends in a large extent on how well preliminary testing goes; that is, how well does the CBR system actually predict user preferences. Depending on the results of these tests, several possible directions were deemed possible:

\begin{enumerate}
\item Given test failure, making improvements to the algorithm, hereunder, the retrieval methods, the amount of data stored per case, the distance metric used to compare cases, and the weighting of the different features of a case/policy.
\item Extending the system by implementing the community portion/collaborative filtering part of the proposed system.
\item Given a test success, implementing a working browser plugin.
\item Closely related to the previous point, extending the system to work with other privacy policy standards.
\end{enumerate}

Direction 1 above takes the project from a system engineering direction more towards a scientific and statistical analysis type project, and while placing some emphasis on tuning the algorithms, this is not our primary focus\footnote{This would require gathering a dataset of some size as well as setting up testing scenarios, which not only requires a sophisticated statistics background, but also likely group of test users. It was decided that this should not be prioritized in a software engineering project of limited scope such as this.}. Item 3 relies heavily on the effectiveness of the algorithm and may require exactly the type of statistical analysis previously mentioned. In agreement with the customer, it was therefore concluded that the collaborative filtering part was to be the second focus after the CBR part. 

\section{Development Model and Workflow}
Having briefly identified and outlined the project "flow", a development method or model must be chosen. A two-stage implementation process is envisioned.

The first stage implements the core CBR.  In the second part, the focus is shifted to the community/CF portion. By splitting the work in two stages, time is made available for more thorough testing and reworking of the CBR portion, while work on the CF system, which is given a lower priority is pushed back. Given this project flow, we deemed that the work fits well within waterfall model framework. This is described in more detail in the next section. 


\subsection{The Waterfall Model}\index{Waterfall Model}
The waterfall model describes the development process as a linear process that starts with planning and requirements specification, and flows sequentially through design, implementation and finally (in this case) testing as is illustrated in Figure~\ref{waterfall}. Arguments \emph{for} the waterfall approach are that it encourages rigorous planning and design, which means that inconsistencies and problems can be discovered earlier in the process, which is generally less costly than if they are discovered late, since this often means re-writing a large portion of code. 

Another advantage of the waterfall model, which relates directly to the nature of this particular project, is the focus on design. Since the software product to be delivered is a very early prototype that is to be used in further research, and likely to be further modified in the future, providing solid modularity and interfaces so as to allow code reuse is critical. Properly documenting the program structure, as encouraged by the waterfall model, will also be highly beneficial to anyone who is later to modify the program.

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{PrelimStudy/workflow}
    \caption{The Waterfall Model.}
    \label{workflow}
  \end{center}
\end{figure}

A common criticism raised against the waterflow model is that development rarely occurs in completely distinct stages; it is often hard to completely finish one phase of the project before moving on to the next. For instance, in many projects, requirements may be subject to uncertainty or change, so that design in turn must be altered. While recognizing this and other inherent weaknesses in the waterflow model, we think that for this project, requirements are quite clear, and since its scope is limited, a slightly modified waterfall model will still serve the project well. 

Among the modifications that we have made is, as indicated in Figure~\ref{workflow}, a slight overlap between the different phases. This is justified by the fact that some stages are not dependent entirely on the previous stage. For instance, given a detailed design, a test plan and testing procedures can be worked out independently, and testing of one module does not require the entire system to be integrated.



\section{Development Tools and Technologies}\label{DevTools}
This section details some of the choices that were made regarding development tools and technologies for this project.

\subsection{Documents and Source Code Repositories}
For software projects of a certain scale, version control is an important technology that allows project members to work simultaneously against the same files without causing inconsistencies. Version control systems also allow for comparison with older versions, tracking changes and restoring previous copies in the case of errors.

For source code repositories and version control, \textbf{Git} was selected. Git is a distributed, open source version control system that is available for all platforms. Git is also used for hosting the \LaTeX documents that comprise this report. For other documents such as meeting minutes, agendas, status reports, time reporting and certain planning documents, \textbf{Google Docs} is used.


\subsection{Programming Languages}
\textbf{Java} is chosen as the primary programming language for implementing the majority of the code. Java is an object oriented programming language providing a level of abstraction appropriate for the task at hand in addition to a rich set of libraries, including the SWING library for GUI programming, and several libraries for networking. It also simplifies writing a browser plugin as major browsers such as Google Chrome and Firefox employ Javascript as scripting language for plugin.

Javascript was used in small measure for the use of a CouchDB community server.

\subsection{Development Tools}
Eclipse was used for all Java development, while standard text editors were used for Javascript and manual P3P policy editing. The CouchDB server was hosted on a VMware virtual machine, and was accessible via web interface on most standard web browsers.

\subsection{Databases}
For testing purposes, it has been decided on using flat file storage of privacy policy data using Java's \texttt{Serializable} interface. However, the output functionality is to be written in a generic fashion to simplify use of database systems such as MySQL, CouchDB and so forth. 

For the collaborative filtering it was decided on doing further research along the lines of using either MySQL or CouchDB, but this work will be pushed forward in time as the key success factor for this project is the CBR engine. The collaborative filtering part will likely be worked on by spare resources in later project phases, and their background and competences will influence the choice of database platform.

\subsection{Third Party Libraries}
 
For developing the CBR as well as P3P parsing components of the Privacy Advisor, a decision had to be made regarding the usage of third party libraries, either for components or for the entire CBR system. Two options were considered with respect to the CBR system. The first was to use a full third party CBR system (jColibri). The alternative was to use a third party system for the retrieval component of the CBR system (i.e. a k Nearest Neighbors (kNN) implementation).


\subsubsection{Third Party CBR System}

The customer, SINTEF ICT, suggested looking into an open source CBR library developed at the Universidad Complutense de Madrid.
 
jColibri is a CBR system that has been under development for well over 10 years and is a very comprehensive system allowing for database interfaces and several other features,  and is according to the customer, a popular choice in academia for CBR projects. It is also written in Java, which of course makes interfacing it simple from our own Java project.
 
However, its comprehensiveness also means that it takes more reading to understand and properly apply to the project at hand, and due to its size and poor documentation, jColibri was ultimately deemed unfit for the Privacy Advisor project. Due to the limited time resources available to this project, the risks associated with spending a large amount of time on a third party library that eventually would not be running was to high.
 
\subsubsection{Third Party k Nearest Neighbors Implementations}
Since kNN is a standard classification algorithm, there are several open source implementations available. Limiting the search space to Java implementations, a library called \emph{The Java Machine Learning Library} (JavaML) was the primary candidate, as it provided a clean and simple interface and allowed for extracting confidence measures.
 
The problem with this library relates to the nature of distance metrics used in classifying privacy policies which is compositional in a way that is non-trivial to handle in JavaML. Furthermore, JavaML seems to operate only on arrays of floating point numbers, which means the distance metric must be defined in two stages; first mapping from \emph{policy domain} to real numbers, then in terms of a metric on real vectors.

\subsubsection{P3P/XML Parser}
Looking for XML parsers on the Java platform, we found out that there are two different types of XML parsers we could use, the first being a DOM Parsers and the second one being a sequential access parser. The difference being that DOM parsers operate on the document as a whole, while sequential access parsers operates on each piece of the XML document sequentually.

We ended up using SAXParser, an internal sequential access parsers in Java. The task from here was to implement it, making the policy as an object with the fields of our choosing. It works by sequentially going through all elements of the XML document, and with easy string comparison, checking if the element is of the wanted ones.

\subsubsection{Networking Software}

\subsubsection{Third Party Licences}
All the third party libraries used by the Privacy Advisor Project use the Apache v2 licence\footnote{See: http://www.apache.org/licenses/LICENSE-2.0.html.}.


\section{Standards}
To achieve clean and reusable code, the project has adopted Oracle's Coding Conventions for the Java Programming Language\footnote{http://www.oracle.com/technetwork/java/codeconvtoc-136057.html}. This is mentioned in the requirements specification due to the high likelihood of the customer having to change the source code for later adaptations.
