% Activate the following line by filling in the right side. If for example the name of the root file is Main.tex, write
% "...root = Main.tex" if the chapter file is in the same directory, and "...root = ../Main.tex" if the chapter is in a subdirectory.
 
%!TEX root =  

\chapter{Test plan}

\minitoc

\subsection*{Test plan}
This is the test plan for the "Privacy Advisor" application requested by SINTEF ICT. This test plan is based on IEEE829-1998, the IEEE standard for software test documentation, with some adaptions to fit this project better. The purpose of testing is to find bugs and errors and correct them, and to make sure the program is working as expected. The purpose of this test plan is to make sure the tests will be executed as planned, and that they are well documented.

\setcounter{tocdepth}{1}

\section{Test Methods}\index{Test methods}
There are two main types of software testing: black-box testing and white-box testing.
		
\subsection{Black-box testing}\index{Test methods!Black-box testing}
This is a method that test the functionality of an application. For this type of testing, knowledge about the application's code and structure is not required. The test cases are based on external descriptions of the software, e.g. specifications, functional requirements or designs. Black-box tests are usually functional, but they can also be non-functional. This type of testing can be applied to all levels of software testing.
		
\subsection{White-box testing}\index{Test methods}\index{Test methods!White-box testing}
This method is used for testing internal structures of an application. For white-box testing it is required to have both knowledge about the code and structure of the application, as well as knowledge about programming to design the test cases. This type of testing is normally done at unit level where it can test paths within a unit or paths between units, but it can also be used at integration and system levels of testing. This method can uncover many errors and problems, but it is not a good test method with regards to finding out whether the program is fulfilling the requirements or not.

\section{Testing approach}
This is a program that is going to be used for research, which means that black-box testing will not be very useful as the client want to work on and test algorithms themselves. Our main task is to deliver a good framework with the necessary tools, and a working, learning algorithm so that further testing can be done with ease. Therefore the focus will be on testing the modules, that is, white-box testing. Another consequence of the requirement for high modularity is that it will be a goal to have the tests be as little dependent on other modules as possible. This is to ensure that a module will not fail if another module is replaced. There will be no training needs, as the testers are also involved in the programming.

		\subsection{What will be tested:}
			\begin{itemize}
				\renewcommand{\labelitemi}{$\bullet$}
					\item \textbf{Unit testing:} This will be used for testing the functionality of the modules, so that it can be ensured that they are working as intended and independent of eachother.
					\item \textbf{That the algorithm is learning:} As our algorithm is based on case-based reasoning (CBR), it will be important to test that it is learning from new data.
			\end{itemize}

		\subsection{What will \underline{not} be tested:}
			\begin{itemize}
				\renewcommand{\labelitemi}{$\bullet$}
					\item \textbf{Usability testing:} This program is intended for further research by the client, and not for use by customers. Since this is not a program supposed to be ready for end users, there is no need to perform end-user tests to see how users interact with the 							program, and whether the product is accepted by users or not.
					\item \textbf{Interface testing:} For the same reasons no any tests on the quality of the interfaces will be executed. The interfaces that is included are there to make testing easier for the client, not to provide the best possible interaction with end-users.
					\item \textbf{Run time:} There will be no designated tests for checking and optimizing the run time. This is because the main classification algorithm can be easily changed, and this will not be a problem for reasonably sized datasets. Run time will only be looked into if the program is very slow even for small data sets.
			\end{itemize}
		
	\section{Test case overview}
		This is the test cases and their identifiers. The identifiers are named UNIT-XX, where XX is the number of the test case.
		\vspace{8 mm}
		
		\textbf{Unit tests:}
		\begin{itemize}
			\renewcommand{\labelitemi}{$\bullet$}
				\item UNIT-01: Command line interface (CLI) functionality
				\item UNIT-02: P3P parser
				\item UNIT-03: Local database
				\item UNIT-04: Graphical user interface (GUI) functionality
				\item UNIT-05: Algorithm classification
				\item UNIT-06: Algorithm learning
				\item UNIT-07: Packet passing through network to community database
		\end{itemize}

		Under is the template for the UNIT tests.
		\begin{center}
			%\begin{table}[h!]
			\begin{tabular}{ |  p{4cm} | p{10cm} | }
				\hline
				Item & Description \\ [3pt] \hline \hline
				Name & The name of the test \\  [3pt] \hline
				Test identifier & The identifier of the test \\  [3pt] \hline
				Person responsible & The person responsible for making sure the test is executed correctly and on time. \\  [3pt] \hline
				Feature(s) to be tested & What kind of functionality that is being tested. \\  [3pt] \hline
				Pre-conditions & What code and environment that has to be in place before the test can be executed. \\  [3pt] \hline
				Execution steps & Stepwise explanation of how to perform the test. \\  [3pt] \hline
				Expected result & The expected output/result for each step for the test to be successful. \\  [3pt] \hline
			\end{tabular}
			%\caption{Template for unit tests}
			%\end{table}
		\end{center}

	\section{Test cases}
		See appendix C for the test cases.

	\section{Test case execution}
		See appendix D for the test case execution documentation.

	\section{Test pass / fail criteria}
		A test is passed if the given execution steps and input produce the expected results. If they do not, the test is failed.

	\section {Test schedule}
		Under is the table for when testing for each of the UNIT tests are scheduled to start.

		\begin{center}
			%\begin{table}[h!]
			\begin{tabular}{ |  p{5cm} | p{5cm} | }
				\hline
				Test identifier & Execution date \\ [3pt] \hline \hline
				UNIT-01 & October 22nd \\  [3pt] \hline
				UNIT-02 & October 24th \\  [3pt] \hline
				UNIT-03 & October 26th \\  [3pt] \hline
				UNIT-04 & October 29th \\  [3pt] \hline
				UNIT-05 & October 29th \\  [3pt] \hline
				UNIT-06 & October 29th \\  [3pt] \hline
				UNIT-07 & November 8th \\  [3pt] \hline
			\end{tabular}
			%\caption{Scheduled start dates for the UNIT tests}
			%\end{table}
		\end{center}

\section{Risks and contingencies}
For some of the tests it is not possible to test every combination of input and output. So there is a chance a test will pass for all the combinations that have been tested for a specific test case, but still fail at some later point for some other combination. This is a risk for the tests UNIT-02 P3P parser, and UNIT-05 Algorithm classification.

The P3P policies have a huge variety in what elements they contain, and certain elements have a N-to-1 relation, so it will be impossible to test if everything is parsed correctly for every possible P3P policy. The best way to prevent this is to handpick a set of policies that have as different content as possible, so that as many as possible of the extremes will be covered.

This is also an issue for testing the algorithm classification. There is simply too many combinations of learning base and input to cover every combination. So again the tests have to be executed with regards to covering as many extremes as possible to reduce the risk of unnoticed bugs.

\section{Testing evaluation}
The first test was started a few days after schedule due to sickness, though without causing any problems. The networking test (UNIT-07) also get delayed a few days as the decision of implementing this was made late, combined with some problems with getting it working properly. There were  some problems with creating the JUnit tests, as none of the groupmembers had any previous experience with this, but this only caused extra work and not delay compared to the schedule. The main reason this was not a problem is that most of the tests did not require, or could not be tested using JUnit. The two tests that caused the most work were the tests for the command line and graphical user interfaces, UNIT-01 and UNIT-04 respectively. Alot of bugs were found here, and some parts of the tests had to be delayed because of code not being fully implemented. These tests were also ran several times to make sure none of the previously working sections had broken during updates and refactoring.

The risk factor in the test for the P3P parser, UNIT-02, was considered as the test was run nine times on different policies. It was attempted to choose different policies to parse, from few to many cases, and with different contents in the cases. All of these nine tests were run successfully, with no bugs in the parser or missing elements. The reason this test took so long from start to finish, is that the expiry date was not parsed at all for any policy. This was a very low priority issue so it did not get fixed quickly.
Algorithm classification, UNIT-05, was not that extensive as alot more manual calculations are needed for each test. Still a few different combinations of policies and history were tested, in addition to making sure the algorithm produces reasonable results for special situations, e.g. where the algorithm tries to match a policy with itself.
All in all these two tests were covered well, and the risk for unnoticed bugs is very small.
